{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb710449",
   "metadata": {},
   "source": [
    "# IO #\n",
    "IO en polars, hace referencia a la entrada y salida (Input/Output) para leer y escribir datos en diferentes formatos.\n",
    "Tales como CSV, JSON, Parquet y más. Los métodos de IO de Polars se pueden utilizar para cargar datos desde archivos en diferentes formatos y también para escribir los resultados de tus operaciones en un formato deseado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc76b24d",
   "metadata": {},
   "source": [
    " # 1. CSV #\n",
    "Un archivo CSV (Comma-Separated Values) es un formato que se utiliza para almacenar datos tabulares, donde cada línea del archivo representa una fila y los valores de cada fila están separados por comas u otros delimitadores. Este tipo de formato se utiliza ampliamente para intercambiar datos entre diferentes aplicaciones, ya que fácilmente legible y fácil de procesar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a14c0",
   "metadata": {},
   "source": [
    "## 1.1 Lectura, Escritura y Escaneo ##\n",
    "En polars tenemos tres métodos de lectura y escritura de archivos CSV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b11ea",
   "metadata": {},
   "source": [
    "### 1.1.1 Lectura ###\n",
    "Para leer un archivo CSV se usa el método *read_csv*. Tal como se ve en el siguiente ejemplo, usando el data sets *iris*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf259945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df = pl.read_csv('iris.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b019dd09",
   "metadata": {},
   "source": [
    "### 1.1.2 Escritura ###\n",
    "Para escribir un DataFrame en un archivo CSV utilizando Polars, puedes utilizar el método *write_csv*. Tal como se ve en el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23169174",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame({\"notas\": [0, 1, 2, 3,4,5,6], \"Estudiantes\": [\"Ana\", \"Beto\", \"Carla\", \"Dante\", \"Elena\", \"Franck\", \"Galia\"]})\n",
    "df.write_csv(\"relacion2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d43b5f",
   "metadata": {},
   "source": [
    "### 1.1.3 Escanear ###\n",
    "Escanear un archivo CSV, permite una manipulación más avanzada de los datos antes de convertirlos en un DataFrame. Para esto usamos el método *scan_csv*. Como en el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6290e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.scan_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe541a",
   "metadata": {},
   "source": [
    "# 2. Parquet #\n",
    "Parquet es un formato de archivo columnar diseñado para el almacenamiento eficiente y el procesamiento de datos. En Polars, se puede trabajar con archivos  de este tipo utilizando las funciones proporcionadas por la biblioteca."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382af31",
   "metadata": {},
   "source": [
    "## 2.1 Lectura, Escritura y Escaneo ##\n",
    "\n",
    "En Polars, se puede leer, escribir y escanear archivos Parquet utilizando las funciones proporcionadas por la biblioteca."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc06d8",
   "metadata": {},
   "source": [
    "### 2.1.1 Lectura ###\n",
    "Lee un archivo Parquet utilizando la función *read_parquet*. Como en el siguiente ejemplo: En el cuál usaremos un archivo test con extensión parquet. Obtenida de Kaggle, en la siguiente dirección web https://www.kaggle.com/datasets/alvinleenh/tps-rocket-league-data-float16-parquet-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32c8bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pl.read_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e24fb",
   "metadata": {},
   "source": [
    "### 2.1.2 Escritura ###\n",
    "Para escribir un DataFrame en un archivo Parquet, utilizamos el método *write_parquet*. Tal como se ve en el siguiente ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b809b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pl.DataFrame({\"notas\": [0, 1, 2, 3,4,5,6], \"Estudiantes\": [\"Ana\", \"Beto\", \"Carla\", \"Dante\", \"Elena\", \"Franck\", \"Galia\"]})\n",
    "df3.write_parquet(\"test2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abce35d2",
   "metadata": {},
   "source": [
    "### 2.1.3 Escaneo ###\n",
    "Para escanear y manipular un archivo Parquet sin cargarlo en un DataFrame, se puede utilizar el método *scan_parquet*. La forma de usarse, se puede ver con el siguiente ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95574da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pl.scan_parquet(\"test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba868e4",
   "metadata": {},
   "source": [
    "# 3. Archivos JSON #\n",
    "Los archivos JSON (JavaScript Object Notation) son un formato de almacenamiento y transmisión de datos basado en texto. Son ampliamente utilizados para representar y transmitir datos estructurados en aplicaciones web y sistemas de intercambio de datos.\n",
    "\n",
    "En un archivo JSON, los datos se organizan en una estructura jerárquica de pares clave-valor. Estos pares clave-valor pueden contener diferentes tipos de datos, como cadenas de texto, números, booleanos, objetos anidados y listas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add5fe04",
   "metadata": {},
   "source": [
    "## 3.1 Lectura, Escritura y Escaneo\n",
    "Aunque Polars no tiene un formato de archivo JSON específico, puedes leer y escribir datos en formato JSON utilizando las funciones proporcionadas por la biblioteca. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb4482",
   "metadata": {},
   "source": [
    "### 3.1.1 Lectura ###\n",
    "Para leer un archivo JSON, se utiliza el método \"read_json\". Cabe señalar que el archivo json debe tener un formato array para ser leído."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7f89a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pl.read_json(\"jsonprueba.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9342af",
   "metadata": {},
   "source": [
    "### 3.1.2 Escritura ###\n",
    "Para escribir un DataFrame en un archivo JSON, utiliza el método *write_json*. Tal como se puede ver en el siguiente ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3f904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame({\"notas\": [0, 1, 2, 3,4,5,6], \"Estudiantes\": [\"Ana\", \"Beto\", \"Carla\", \"Dante\", \"Elena\", \"Franck\", \"Galia\"]})\n",
    "# guardando en formato json\n",
    "df.write_json(\"relacion2.json\")\n",
    "# guardando en formato ndjson\n",
    "df.write_ndjson(\"relacion2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace46b63",
   "metadata": {},
   "source": [
    "### 3.1.3 Escaneo ###\n",
    "Polars permite escanear una entrada JSON, solo para json delimitado por saltos de línea . El escaneo retrasa el análisis real del archivo y, en su lugar, devuelve un contenedor de cálculo diferido llamado *LazyFrame*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b5f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.scan_ndjson(\"relacion2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9891973",
   "metadata": {},
   "source": [
    "# 4. Múltiple #\n",
    "En Polars, el término \"multiple\" se utiliza para referirse a varias operaciones o funciones aplicadas a múltiples columnas o filas de un DataFrame al mismo tiempo. Esto puede incluir operaciones matemáticas, transformaciones, filtrado de datos, agregación, entre otros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07598d39",
   "metadata": {},
   "source": [
    "## 4.1 Manejo de varios archivos ##\n",
    "En Polars, puedes manejar varios archivos utilizando las funciones proporcionadas para lectura y escritura de datos. Considerando las necesidades y capacidad de la memoria. En el siguiente ejemplo se tiene un código el cuál a partir de un dataframe, se puede obtener cinco archivos en formato csv, cada cuál con su respectivo nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8dbbc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame({\"notas\": [0, 1, 2, 3,4,5,6], \"Estudiantes\": [\"Ana\", \"Beto\", \"Carla\", \"Dante\", \"Elena\", \"Franck\", \"Galia\"]})\n",
    "\n",
    "for i in range(5):\n",
    "    df.write_csv(f\"archivo{i}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c63f7",
   "metadata": {},
   "source": [
    "## 4.2 Lectura de un solo *DataFrame* ##\n",
    "Se puede leer varios archivos en un solo DataFrame, para esto se hace uso de patrones globales:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f069a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta que contiene los archivos CSV\n",
    "folder_path = \"http://localhost:8888/tree/OneDrive/Escritorio/varios\"\n",
    "\n",
    "# Leer todos los archivos CSV dentro de la carpeta\n",
    "df = pl.read_csv(folder_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47d0b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "df12 = pl.read_csv(\"archivo*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9f1af6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (35, 2)\n",
      "┌───────┬─────────────┐\n",
      "│ notas ┆ Estudiantes │\n",
      "│ ---   ┆ ---         │\n",
      "│ i64   ┆ str         │\n",
      "╞═══════╪═════════════╡\n",
      "│ 0     ┆ Ana         │\n",
      "│ 1     ┆ Beto        │\n",
      "│ 2     ┆ Carla       │\n",
      "│ 3     ┆ Dante       │\n",
      "│ …     ┆ …           │\n",
      "│ 3     ┆ Dante       │\n",
      "│ 4     ┆ Elena       │\n",
      "│ 5     ┆ Franck      │\n",
      "│ 6     ┆ Galia       │\n",
      "└───────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(df12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f5e349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import graphviz\n",
    "#pl.scan_csv(\"archivo*.csv\").show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a14eda",
   "metadata": {},
   "source": [
    "## 4.3 Lectura y procesamiento paralelo\n",
    "Si el objetivo es que los archivos no tengan que estar en una sola tabla, también se puede crear un plan de consulta para cada archivo y ejecutarlos en paralelo como grupos de subprocesos.\n",
    "\n",
    "Toda la ejecución del plan de consulta es paralela y no requiere ninguna comunicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de304102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shape: (7, 3)\n",
      "┌───────┬───────┬─────────────┐\n",
      "│ notas ┆ count ┆ Estudiantes │\n",
      "│ ---   ┆ ---   ┆ ---         │\n",
      "│ i64   ┆ u32   ┆ str         │\n",
      "╞═══════╪═══════╪═════════════╡\n",
      "│ 5     ┆ 1     ┆ null        │\n",
      "│ 6     ┆ 1     ┆ null        │\n",
      "│ 0     ┆ 1     ┆ null        │\n",
      "│ 1     ┆ 1     ┆ null        │\n",
      "│ 3     ┆ 1     ┆ null        │\n",
      "│ 2     ┆ 1     ┆ null        │\n",
      "│ 4     ┆ 1     ┆ null        │\n",
      "└───────┴───────┴─────────────┘, shape: (7, 3)\n",
      "┌───────┬───────┬─────────────┐\n",
      "│ notas ┆ count ┆ Estudiantes │\n",
      "│ ---   ┆ ---   ┆ ---         │\n",
      "│ i64   ┆ u32   ┆ str         │\n",
      "╞═══════╪═══════╪═════════════╡\n",
      "│ 4     ┆ 1     ┆ null        │\n",
      "│ 5     ┆ 1     ┆ null        │\n",
      "│ 1     ┆ 1     ┆ null        │\n",
      "│ 2     ┆ 1     ┆ null        │\n",
      "│ 3     ┆ 1     ┆ null        │\n",
      "│ 6     ┆ 1     ┆ null        │\n",
      "│ 0     ┆ 1     ┆ null        │\n",
      "└───────┴───────┴─────────────┘, shape: (7, 3)\n",
      "┌───────┬───────┬─────────────┐\n",
      "│ notas ┆ count ┆ Estudiantes │\n",
      "│ ---   ┆ ---   ┆ ---         │\n",
      "│ i64   ┆ u32   ┆ str         │\n",
      "╞═══════╪═══════╪═════════════╡\n",
      "│ 6     ┆ 1     ┆ null        │\n",
      "│ 2     ┆ 1     ┆ null        │\n",
      "│ 0     ┆ 1     ┆ null        │\n",
      "│ 1     ┆ 1     ┆ null        │\n",
      "│ 3     ┆ 1     ┆ null        │\n",
      "│ 5     ┆ 1     ┆ null        │\n",
      "│ 4     ┆ 1     ┆ null        │\n",
      "└───────┴───────┴─────────────┘, shape: (7, 3)\n",
      "┌───────┬───────┬─────────────┐\n",
      "│ notas ┆ count ┆ Estudiantes │\n",
      "│ ---   ┆ ---   ┆ ---         │\n",
      "│ i64   ┆ u32   ┆ str         │\n",
      "╞═══════╪═══════╪═════════════╡\n",
      "│ 0     ┆ 1     ┆ null        │\n",
      "│ 2     ┆ 1     ┆ null        │\n",
      "│ 4     ┆ 1     ┆ null        │\n",
      "│ 6     ┆ 1     ┆ null        │\n",
      "│ 1     ┆ 1     ┆ null        │\n",
      "│ 5     ┆ 1     ┆ null        │\n",
      "│ 3     ┆ 1     ┆ null        │\n",
      "└───────┴───────┴─────────────┘, shape: (7, 3)\n",
      "┌───────┬───────┬─────────────┐\n",
      "│ notas ┆ count ┆ Estudiantes │\n",
      "│ ---   ┆ ---   ┆ ---         │\n",
      "│ i64   ┆ u32   ┆ str         │\n",
      "╞═══════╪═══════╪═════════════╡\n",
      "│ 5     ┆ 1     ┆ null        │\n",
      "│ 6     ┆ 1     ┆ null        │\n",
      "│ 4     ┆ 1     ┆ null        │\n",
      "│ 1     ┆ 1     ┆ null        │\n",
      "│ 2     ┆ 1     ┆ null        │\n",
      "│ 3     ┆ 1     ┆ null        │\n",
      "│ 0     ┆ 1     ┆ null        │\n",
      "└───────┴───────┴─────────────┘]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "queries = []\n",
    "for file in glob.glob(\"archivo*.csv\"):\n",
    "    q = pl.scan_csv(file).groupby(\"notas\").agg([pl.count(), pl.sum(\"Estudiantes\")])\n",
    "    queries.append(q)\n",
    "\n",
    "dataframes = pl.collect_all(queries)\n",
    "print(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b24dfa7",
   "metadata": {},
   "source": [
    "# 5. Leer de una base de datos #\n",
    "Se puede leer de una base de datos con Polars usando pl.read_databasefunción. Para usar esta función, necesita una cadena de consulta SQL y una cadena de conexión llamada connection_uri.\n",
    "\n",
    "Por ejemplo, en el siguiente código se muestra los patrones generales para leer todas las columnas de una tabla en una base de datos de Postgres:\n",
    "\n",
    "```python\n",
    "connection_uri = \"postgres://username:password@server:port/database\"\n",
    "query = \"SELECT * FROM foo\"\n",
    "\n",
    "pl.read_database(query=query, connection_uri=connection_uri)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df09f330",
   "metadata": {},
   "source": [
    "## 5.1 Motores ##\n",
    "Polars no gestiona las conexiones y la transferencia de datos desde las bases de datos por sí mismo. En cambio, las bibliotecas externas (conocidas como motores ) manejan esto. En la actualidad Polars puede utilizar dos motores para leer bases de datos:    1. ConectorX y\n",
    "2. ADBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a5c611",
   "metadata": {},
   "source": [
    "### 5.1.1 Conector - x ###\n",
    "ConnectorX es el motor predeterminado y admite numerosas bases de datos, incluidas Postgres, Mysql, SQL Server y Redshift. ConnectorX está escrito en Rust y almacena datos en formato Arrow para permitir la copia cero en Polars.\n",
    "\n",
    "Para leer desde una de las bases de datos compatibles, *ConnectorX* debe activar la dependencia adicional *ConnectorX* al instalar Polars o instalarla manualmente con\n",
    "$  pip install connectorx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d1806",
   "metadata": {},
   "source": [
    "### 5.1.2 ADBC ###\n",
    "ADBC (Arrow Database Connectivity) es un motor compatible con el proyecto Apache Arrow. ADBC pretende ser un estándar API para conectarse a bases de datos y bibliotecas que implementen este estándar en una variedad de idiomas.\n",
    "\n",
    "Todavía es pronto para ADBC, por lo que el soporte para diferentes bases de datos aún es limitado. Actualmente, los controladores para ADBC ​​solo están disponibles para Postgres y SQLite . Para instalar ADBC ​​necesita instalar el controlador para su base de datos. Por ejemplo, para instalar el controlador para SQLite que ejecuta.\n",
    "$  pip install adbc-driver-sqlite\n",
    "Como ADBC no es el motor predeterminado, debe especificar el motor como argumento *parapl.read_database*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a24225",
   "metadata": {},
   "source": [
    "# 6. AWS #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b10cc",
   "metadata": {},
   "source": [
    "# 7. Google Big Query #\n",
    "Google BigQuery es un servicio de análisis de datos en la nube de Google que permite almacenar, consultar y analizar grandes conjuntos de datos utilizando el poder del procesamiento distribuido. Para se necesitan dependencias adicionales:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a09d0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
